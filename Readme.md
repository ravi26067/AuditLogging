# Audit Logging System

## Overview
This project implements an audit logging system using Kafka, Redis, Elasticsearch, and Flask. The system consists of a Kafka consumer that reads events from a Kafka topic, processes them, and stores them in Elasticsearch. A Flask API is provided to query the stored events with various filters and caching support via Redis.

## Project Structure
```
audit-logging-system/
├── docker-compose.yml
├── Dockerfile
├── event_consumer/
│   ├── __init__.py
│   ├── consumer.py
│   └── config.py
├── api/
│   ├── __init__.py
│   ├── app.py
│   ├── config.py
│   └── requirements.txt
├── tests/
│   ├── test_consumer.py
│   └── test_api.py
├── scripts/
│   └── setup_kafka.sh
└── README.md
```

## Setup Instructions

### Prerequisites
- Docker
- Docker Compose
- Python 3.9
- pip
- Visual Studio Code (VS Code) for development

### Step 1: Start Services with Docker Compose
1. Open a terminal and navigate to the project directory.
2. Run the following command to start the services:
   ```sh
   docker-compose up -d
   ```

### Step 2: Set Up Kafka Topic
1. Run the setup script to create the Kafka topic:
   ```sh
   bash scripts/setup_kafka.sh
   ```

### Step 3: Run Kafka Consumer
1. Open a terminal and navigate to the project directory.
2. Run the consumer script:
   ```sh
   python event_consumer/consumer.py
   ```

### Step 4: Run Flask API
1. Open another terminal and navigate to the project directory.
2. Install dependencies:
   ```sh
   pip install -r api/requirements.txt
   ```
3. Run the Flask API:
   ```sh
   python api/app.py
   ```

### Testing
- To run the tests, navigate to the `tests/` directory and run:
  ```sh
  python -m unittest test_consumer.py
  python -m unittest test_api.py
  ```

### API Usage
- You can test the API by accessing the following endpoint:
  ```
  GET /events?actor=<actor>&action=<action>&resource=<resource>&tenant_id=<tenant_id>&time_from=<time_from>&time_to=<time_to>
  ```
  Example:
  ```
  GET /events?actor=john_doe&action=user_logged_in
  ```

### File Explanations

#### Docker Compose File (`docker-compose.yml`)
Defines the services required for the system: Zookeeper, Kafka, Redis, Elasticsearch, Flask API, and Kafka consumer.

#### Dockerfile (`Dockerfile`)
Defines the base image and setup instructions for the Docker container running the Flask API.

#### Event Consumer (`event_consumer/`)
- `__init__.py`: Initializes the package.
- `consumer.py`: Contains the Kafka consumer logic.
- `config.py`: Configuration settings for Kafka, Elasticsearch, and Redis.

#### Flask API (`api/`)
- `__init__.py`: Initializes the package.
- `app.py`: Contains the Flask application with endpoints.
- `config.py`: Configuration settings for Elasticsearch and Redis.
- `requirements.txt`: Lists the dependencies for the Flask application.

#### Tests (`tests/`)
- `test_consumer.py`: Tests the Kafka consumer connections.
- `test_api.py`: Tests the Flask API endpoints.

#### Scripts (`scripts/`)
- `setup_kafka.sh`: Script to set up Kafka topics.

## High-Level Design (HLD)

### Overview

The audit logging system is designed to track all operations performed by users on Acme Corp's bug tracking product. The system reads events from a Kafka event bus, performs basic transformations, stores the events in Elasticsearch, and allows retrieval based on various filters. The system is designed to be multi-tenant, scalable, and highly available.

### Architecture Components

1. **Kafka Event Bus:**
   - **Purpose:** Acts as the source of truth for user activities. It receives and queues events generated by user actions in the bug tracking system.
   - **Scalability:** Handles high throughput with the capability of processing 1000 messages per second.
   - **Availability:** Kafka ensures high availability and fault tolerance by distributing logs over multiple brokers.

2. **Kafka Consumer:**
   - **Purpose:** Consumes events from Kafka, processes them, and stores them in Elasticsearch.
   - **Implementation:** Python-based consumer service that listens to Kafka topics, transforms events, and indexes them in Elasticsearch.
   - **Scalability:** Can be scaled horizontally by adding more consumer instances.
   - **Error Handling:** Implements retry logic and error logging for robust processing.

3. **Elasticsearch:**
   - **Purpose:** Acts as the primary data store for storing and querying event data.
   - **Advantages:** Provides powerful full-text search capabilities, supports complex querying, and allows near real-time search.
   - **Scalability:** Elasticsearch clusters can be scaled horizontally by adding more nodes.
   - **Availability:** Configured for high availability with replicas and fault-tolerant architecture.

4. **Redis (Optional Cache Layer):**
   - **Purpose:** Caches frequently accessed query results to improve read performance and reduce the load on Elasticsearch.
   - **Advantages:** Provides in-memory data storage for ultra-fast data access.
   - **Scalability:** Can be scaled by partitioning data across multiple Redis instances.
   - **Availability:** Supports replication and persistence for high availability.

5. **Flask API:**
   - **Purpose:** Provides a RESTful API interface for querying the stored events based on actor, action, resource, and time filters.
   - **Implementation:** Python-based Flask application exposing endpoints for querying and filtering event data.
   - **Scalability:** Can be deployed behind a load balancer to handle concurrent API requests.
   - **Error Handling:** Implements input validation, error handling, and logging.

### Data Flow

1. **Event Generation:**
   - User actions generate events in the bug tracking system.
   - Events are published to Kafka topics.

2. **Event Consumption:**
   - The Kafka consumer service subscribes to Kafka topics.
   - Events are consumed, transformed, and prepared for storage.

3. **Event Storage:**
   - Transformed events are indexed in Elasticsearch.
   - Optionally, frequently accessed data can be cached in Redis for faster retrieval.

4. **Data Retrieval:**
   - The Flask API receives query requests.
   - Queries are executed against Elasticsearch (and Redis if caching is enabled).
   - Results are returned to the client.

### Design Considerations

1. **Simplicity:** The design prioritizes simplicity with well-defined, loosely coupled components to ensure ease of development, testing, and maintenance.
2. **Scalability:** Each component can be scaled independently to handle increased load, ensuring the system can grow with user demand.
3. **Availability:** High availability is achieved through redundant Kafka brokers, Elasticsearch replicas, and optional Redis replication.
4. **Monitoring and Visibility:** Monitoring tools and logging are integrated to provide visibility into the system's health and performance, allowing proactive issue identification and resolution.

### Architectural Diagram


## Technology Stack
- **Kafka**: For event streaming.
- **Redis**: For caching query results.
- **Elasticsearch**: For storing and querying event data.
- **Flask**: For providing the REST API.

## Author
Ravi Kumar

## Contact
For any questions or suggestions, please contact me at ravi26067@gmail.com.